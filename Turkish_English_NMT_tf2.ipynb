{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gorkemozkaya/nmt-en-tr/blob/master/Turkish_English_NMT_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running predictions with the pre-trained NMT models in Python\n",
        "This notebook illustrates how one can load the pre-trained models shared in this repo and run them on new Turkish or English sentences for translation. The models are trained using the template provided in TensorFlow 2's official models repository.\n",
        "\n",
        "First let's install the compatible versions of the dependencies and clone our repository, which includes the customized `models` and `datasets` packages as a dependency."
      ],
      "metadata": {
        "id": "LzfOiNAjtkjJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA9vR440tW90",
        "outputId": "a48922f8-944e-4448-80b0-c7dcacd809b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 498.0/498.0 MB 1.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 92.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 85.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 kB 4.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 61.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 126.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 462.3/462.3 kB 40.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 85.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 121.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 68.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.2/241.2 kB 15.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.1/175.1 kB 21.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 4.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 45.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.9/118.9 kB 14.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/17.6 MB 77.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.5/304.5 kB 34.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 8.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script sacrebleu is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.0 which is incompatible.\n",
            "tensorflow 2.8.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.3 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%%sh\n",
        "pip install -q tensorflow==2.8.2 tensorflow-text==2.8.2 tensorflow-addons==0.17.1\n",
        "[ -d nmt-en-tr ] || git clone -q --recurse-submodules -j8 https://github.com/gorkemozkaya/nmt-en-tr.git\n",
        "pip3 install -q --user -r /content/nmt-en-tr/models/official/requirements.txt\n",
        "pip3 install -q -e /content/nmt-en-tr/datasets\n",
        "[ -e pretrained_v2.zip ] || wget -nc -q https://github.com/gorkemozkaya/nmt-en-tr/releases/download/pretrained_model_v2/pretrained_v2.zip\n",
        "[ -d pretrained_v2 ] ||  unzip -n -qq pretrained_v2.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update system path"
      ],
      "metadata": {
        "id": "2JQiyzfJO_3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path = ['/content/nmt-en-tr/datasets', '/content/nmt-en-tr/models'] + sys.path\n",
        "sys.path = ['/root/.local/lib/python3.10/site-packages', '/root/.local/bin'] + sys.path"
      ],
      "metadata": {
        "id": "1IRHgFZwuj1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the tokenizer**"
      ],
      "metadata": {
        "id": "_1sgJYOEPGxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y protobuf"
      ],
      "metadata": {
        "outputId": "e2754138-2d10-4c8a-d153-eea9703ba1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdWZ0CBFxHYh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: protobuf 4.23.3\n",
            "Uninstalling protobuf-4.23.3:\n",
            "  Successfully uninstalled protobuf-4.23.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.1"
      ],
      "metadata": {
        "id": "gqw5MAHRxLgz",
        "outputId": "0e72c6c0-e628-42e0-a155-b4126c603e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.20.1\n",
            "  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.20.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-functions 1.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.8.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as tftxt\n",
        "import tensorflow as tf\n",
        "tokenizer= tftxt.SentencepieceTokenizer(\n",
        "          model=tf.io.gfile.GFile(\"pretrained_v2/sentencepiece_en_tr.model\", \"rb\").read(),\n",
        "          add_eos=True)"
      ],
      "metadata": {
        "id": "WjiOBgCsNTVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the keras models"
      ],
      "metadata": {
        "id": "v9Kr2QxHPMpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from official.core import exp_factory, task_factory\n",
        "from official.nlp.configs import wmt_transformer_experiments as wmt_te\n",
        "\n",
        "task_config = exp_factory.get_exp_config('transformer_tr_en_blended/base').task\n",
        "task_config.sentencepiece_model_path = 'pretrained_v2/sentencepiece_en_tr.model'\n",
        "\n",
        "translation_task = task_factory.get_task(task_config)\n",
        "model_en_tr = translation_task.build_model()\n",
        "model_tr_en = translation_task.build_model() # we can use the same task"
      ],
      "metadata": {
        "id": "55-ANSLQPLcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translation wrapper:** Function that does tokenization, translation and detokenization."
      ],
      "metadata": {
        "id": "qlSc5CWHQE6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(input_text, model):\n",
        "  tokenized = tokenizer.tokenize(input_text)\n",
        "  translated = model({'inputs' : tf.reshape(tokenized, [1, -1])})\n",
        "  return tokenizer.detokenize(translated['outputs']).numpy()[0].decode('utf-8')"
      ],
      "metadata": {
        "id": "Ys75ifMRvhGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to do a dry-run before we can load the weights."
      ],
      "metadata": {
        "id": "6-aIw5k8QSbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ignore = translate(\"test\", model_en_tr)\n",
        "ignore = translate(\"test\", model_tr_en)"
      ],
      "metadata": {
        "id": "6WzHml1iQN47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_en_tr.load_weights(\"pretrained_v2/en_tr/en_tr\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB0kJaeoQZNc",
        "outputId": "bb41ceef-38a6-4216-a669-1c1f53ac4e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbb9c219b70>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_tr_en.load_weights(\"pretrained_v2/tr_en/tr_en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYW_4GQVWJLA",
        "outputId": "1e19b9c0-edf2-4fa8-db9f-888829a49d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbaf5ea8280>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"If Turkey provides a competitive, safe, and predictable business and \\\n",
        "investment environment, it can reach high growth rates and development levels, \\\n",
        "with its alternative tourism opportunities, agriculture, young, educated \\\n",
        "population, and entrepreneurial spirit.\"\n",
        "\n",
        "translate(input, model_en_tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "K6Dtw4WtWQW4",
        "outputId": "2fa32c42-a2ef-4ca4-c517-e2f8ed79449e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Türkiye rekabetçi, güvenli ve öngörülebilir bir iş ve yatırım ortamı sağlarsa, alternatif turizm olanakları, tarım, genç, eğitimli nüfus ve girişimci ruhuyla yüksek büyüme oranları ve kalkınma seviyelerine ulaşabilir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"CHP Genel Başkanı Kemal Kılıçdaroğlu, İngiltere'de başbakanlık için \\\n",
        "yarışan Dışişleri Bakanı Liz Truss'ın sığınmacıların Ruanda'ya gönderileceği \\\n",
        "programa Türkiye gibi ülkeleri de ekleyerek genişletmeyi planladığının öne \\\n",
        "sürülmesine tepki gösterdi.\"\n",
        "\n",
        "translate(input, model_tr_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MpwK1vsaWfm-",
        "outputId": "5722e7a1-f247-4865-ca52-2cbae066fc07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Republican People's Party (CHP) Leader Kemal Kilicdaroglu reacted against the idea that Foreign Minister Liz Truss, who competed for prime minister in Britain, planned to expand countries such as Turkey in the programme where asylum seekers will be sent to Rwanda.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yEyNQfTPW9YZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}